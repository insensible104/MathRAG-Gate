"""
Data loading and preprocessing for the MathRAG-Gate project.

This module loads the MATH dataset, forces the cache to be stored in the local
'data/' folder, handles different data formats, and returns LlamaIndex-compatible
documents and test data.
"""

import os
import json
import logging
import sys
from datasets import load_dataset
from llama_index.core.schema import Document
from src.config import settings # ADDED: Use settings for DATA_DIR

# --- Setup Environment Variables (MUST be done before importing datasets) ---

# 1. HF Mirror setup for China users
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_OFFLINE"] = "0" 

# 2. [NEW] Set custom cache directory to the local 'data/' folder
DATA_DIR = settings.DATA_DIR
# Ensure the directory exists
os.makedirs(DATA_DIR, exist_ok=True)
# Set the environment variable for Hugging Face to use this path
os.environ["HF_DATASETS_CACHE"] = os.path.abspath(DATA_DIR) 

# --- Logging Setup ---
# Set up simple logging (needs to be done here or in main.py)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Heuristic Check for Local Existence ---
# Hugging Face caches datasets in a specific subfolder structure (e.g., 'data/DigitalLearningGmbH___MATH-lighteval')
# We use this to provide clear feedback to the user.
DATASET_NAME_PART = "DigitalLearningGmbH___MATH-lighteval"
cache_path_guess = os.path.join(os.environ["HF_DATASETS_CACHE"], DATASET_NAME_PART)
# Check if the expected dataset folder structure exists
IS_DOWNLOADED_LOCALLY = os.path.exists(cache_path_guess)


def load_math_data():
    """
    Loads and preprocesses the MATH dataset.

    - Forces download/cache location to the local 'data/' directory.
    - Returns LlamaIndex Documents for KB + raw strings for test.
    """
    
    # æ ¹æ®æœ¬åœ°æ£€æŸ¥ç»“æœï¼Œå†³å®šåŠ è½½ç­–ç•¥
    if IS_DOWNLOADED_LOCALLY:
        logger.info("âœ… æ£€æµ‹åˆ°æœ¬åœ°æ•°æ®é›†ã€‚æ­£åœ¨ä» 'data/' æ–‡ä»¶å¤¹åŠ è½½ (å¼ºåˆ¶ç¦»çº¿æ¨¡å¼)...")
        # ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨ local_files_only=True å¼ºåˆ¶åªè¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œå®Œå…¨è·³è¿‡ç½‘ç»œæ£€æŸ¥ã€‚
        load_mode_args = {"local_files_only": True}
    else:
        logger.info("ğŸ“š æ­£åœ¨ä» Hugging Face ä¸‹è½½æ•°æ®é›†åˆ° 'data/' æ–‡ä»¶å¤¹ (é¦–æ¬¡è”ç½‘ï¼Œåç»­è‡ªåŠ¨ç¦»çº¿)...")
        # é¦–æ¬¡ä¸‹è½½ï¼šå…è®¸è”ç½‘ï¼Œå¹¶è¦æ±‚å¦‚æœç¼“å­˜å·²å­˜åœ¨åˆ™é‡ç”¨
        load_mode_args = {"download_mode": "reuse_cache_if_exists"}

    try:
        # load_dataset ä¾èµ–äº HF_DATASETS_CACHE env variable
        dataset = load_dataset(
            "DigitalLearningGmbH/MATH-lighteval",
            "default",
            **load_mode_args # åŠ¨æ€ä¼ å…¥åŠ è½½å‚æ•°
        )
    except Exception as e:
        logger.error(f"âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {e}")
        if IS_DOWNLOADED_LOCALLY:
            logger.error("è¯·æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ–‡ä»¶æ˜¯å¦å®Œæ•´æˆ–æŸåã€‚")
        else:
            logger.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ– VPN/ä»£ç†è®¾ç½®æ˜¯å¦ç¨³å®šã€‚")
        raise

    all_items = dataset["train"]
    total = len(all_items)
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {total} æ¡æ•°å­¦é¢˜ç›®")

    # === æ™ºèƒ½è§£æï¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼ï¼ˆdict æˆ– JSON å­—ç¬¦ä¸²ï¼‰===
    # ... (åç»­æ•°æ®å¤„ç†é€»è¾‘ä¿æŒä¸å˜)
    parsed_items = []
    for item in all_items:
        if isinstance(item, dict):
            # å·²æ˜¯å­—å…¸æ ¼å¼
            parsed_items.append(item)
        elif isinstance(item, str):
            # æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            try:
                # Assuming the item is a valid JSON string containing the necessary fields
                parsed_items.append(json.loads(item))
            except (json.JSONDecodeError, TypeError) as parse_err:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ³•è§£æçš„æ¡ç›®: {str(item)[:100]}...")
                continue
        else:
            logger.warning(f"âš ï¸ è·³è¿‡éå­—ç¬¦ä¸²/éå­—å…¸æ¡ç›®: {type(item)}")
            continue

    if not parsed_items:
        raise ValueError("âŒ æ•°æ®é›†ä¸­æ²¡æœ‰æœ‰æ•ˆæ¡ç›®ï¼")

    # === åˆ’åˆ†çŸ¥è¯†åº“ï¼ˆ80%ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ20%ï¼‰===
    split_idx = int(len(parsed_items) * 0.8)
    kb_items = parsed_items[:split_idx]
    test_items = parsed_items[split_idx:]

    # === æ„å»ºçŸ¥è¯†åº“ï¼šè½¬ä¸º LlamaIndex Document ===
    kb_docs = [
        # Combining question and solution provides the best context for retrieval
        Document(text=f"Question: {item['problem']}\nAnswer: {item['solution']}")
        for item in kb_items
        if "problem" in item and "solution" in item
    ]

    # === æ„å»ºæµ‹è¯•é›† ===
    test_questions = [
        item["problem"] for item in test_items
        if "problem" in item and "solution" in item
    ]
    test_answers = [
        item["solution"] for item in test_items
        if "problem" in item and "solution" in item
    ]

    # ç¡®ä¿æµ‹è¯•é›†é•¿åº¦ä¸€è‡´
    min_len = min(len(test_questions), len(test_answers))
    test_questions = test_questions[:min_len]
    test_answers = test_answers[:min_len]

    logger.info(f"âœ… çŸ¥è¯†åº“æ–‡æ¡£æ•°: {len(kb_docs)}")
    logger.info(f"âœ… æµ‹è¯•é›†é—®é¢˜æ•°: {len(test_questions)}")

    return kb_docs, test_questions, test_answers